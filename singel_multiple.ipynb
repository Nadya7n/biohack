{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch import nn\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:19:26.989614Z","iopub.execute_input":"2023-08-12T12:19:26.989981Z","iopub.status.idle":"2023-08-12T12:19:26.995072Z","shell.execute_reply.started":"2023-08-12T12:19:26.989950Z","shell.execute_reply":"2023-08-12T12:19:26.994213Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"!pip install fair-esm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"single = pd.read_csv(\"/kaggle/input/hackaton/train_dataset.csv\", header=None)\nmultiple = pd.read_csv(\"/kaggle/input/hackaton/train_dataset_multiple.csv\", header=None)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:22:12.123607Z","iopub.execute_input":"2023-08-12T12:22:12.123978Z","iopub.status.idle":"2023-08-12T12:22:12.260753Z","shell.execute_reply.started":"2023-08-12T12:22:12.123947Z","shell.execute_reply":"2023-08-12T12:22:12.259559Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([single, multiple])","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:23:22.567110Z","iopub.execute_input":"2023-08-12T12:23:22.567808Z","iopub.status.idle":"2023-08-12T12:23:22.577653Z","shell.execute_reply.started":"2023-08-12T12:23:22.567775Z","shell.execute_reply":"2023-08-12T12:23:22.576581Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"data.columns = [\"wt\", \"mut\", \"score\", \"pos\"]","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:23:31.782223Z","iopub.execute_input":"2023-08-12T12:23:31.782617Z","iopub.status.idle":"2023-08-12T12:23:31.787700Z","shell.execute_reply.started":"2023-08-12T12:23:31.782583Z","shell.execute_reply":"2023-08-12T12:23:31.786491Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import esm","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:24:42.727922Z","iopub.execute_input":"2023-08-12T12:24:42.728307Z","iopub.status.idle":"2023-08-12T12:24:42.732755Z","shell.execute_reply.started":"2023-08-12T12:24:42.728275Z","shell.execute_reply":"2023-08-12T12:24:42.731695Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:24:47.882015Z","iopub.execute_input":"2023-08-12T12:24:47.883014Z","iopub.status.idle":"2023-08-12T12:24:47.888285Z","shell.execute_reply.started":"2023-08-12T12:24:47.882975Z","shell.execute_reply":"2023-08-12T12:24:47.887270Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nTRAIN_SIZE = 20000","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:25:31.192564Z","iopub.execute_input":"2023-08-12T12:25:31.192937Z","iopub.status.idle":"2023-08-12T12:25:31.197813Z","shell.execute_reply.started":"2023-08-12T12:25:31.192900Z","shell.execute_reply":"2023-08-12T12:25:31.196772Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"data = data.sample(frac=1.0, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:25:37.710710Z","iopub.execute_input":"2023-08-12T12:25:37.711087Z","iopub.status.idle":"2023-08-12T12:25:37.724389Z","shell.execute_reply.started":"2023-08-12T12:25:37.711055Z","shell.execute_reply":"2023-08-12T12:25:37.723203Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"train_df = data.iloc[:TRAIN_SIZE, :]\nvalid_df = data.iloc[TRAIN_SIZE:, :]","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:25:43.984256Z","iopub.execute_input":"2023-08-12T12:25:43.984621Z","iopub.status.idle":"2023-08-12T12:25:43.990839Z","shell.execute_reply.started":"2023-08-12T12:25:43.984590Z","shell.execute_reply":"2023-08-12T12:25:43.989417Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"train_df.index = np.arange(0, len(train_df))\nvalid_df.index = np.arange(0, len(valid_df))","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:25:46.787220Z","iopub.execute_input":"2023-08-12T12:25:46.787584Z","iopub.status.idle":"2023-08-12T12:25:46.793514Z","shell.execute_reply.started":"2023-08-12T12:25:46.787553Z","shell.execute_reply":"2023-08-12T12:25:46.792424Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"class Protseq(Dataset):\n    def __init__(self, df):\n        self.df = df\n        _, esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n        self.esm1v_batch_converter = esm1v_alphabet.get_batch_converter()\n\n        \n    def __getitem__(self, idx):\n        _, _, wt = self.esm1v_batch_converter([('' , ''.join(self.df.loc[idx, \"wt\"]))])\n        _, _, mut = self.esm1v_batch_converter([('' , ''.join(self.df.loc[idx, \"mut\"]))])\n        pos = self.df.loc[idx, \"pos\"]\n        target = torch.FloatTensor([self.df.loc[idx, \"score\"]])\n        return wt, mut, pos, target\n\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:25:50.359319Z","iopub.execute_input":"2023-08-12T12:25:50.359692Z","iopub.status.idle":"2023-08-12T12:25:50.370371Z","shell.execute_reply.started":"2023-08-12T12:25:50.359661Z","shell.execute_reply":"2023-08-12T12:25:50.369228Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 180","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:25:51.948730Z","iopub.execute_input":"2023-08-12T12:25:51.949202Z","iopub.status.idle":"2023-08-12T12:25:51.956073Z","shell.execute_reply.started":"2023-08-12T12:25:51.949139Z","shell.execute_reply":"2023-08-12T12:25:51.955062Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"train_ds = Protseq(train_df)\nvalid_ds = Protseq(valid_df)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:25:52.940070Z","iopub.execute_input":"2023-08-12T12:25:52.940501Z","iopub.status.idle":"2023-08-12T12:26:43.675119Z","shell.execute_reply.started":"2023-08-12T12:25:52.940469Z","shell.execute_reply":"2023-08-12T12:26:43.674094Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nvalid_dataloader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:27:02.603377Z","iopub.execute_input":"2023-08-12T12:27:02.603742Z","iopub.status.idle":"2023-08-12T12:27:02.609520Z","shell.execute_reply.started":"2023-08-12T12:27:02.603711Z","shell.execute_reply":"2023-08-12T12:27:02.608089Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"HIDDEN_UNITS_POS_CONTACT = 5\nclass ESM_sum_seqembed(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.esm2, _ = esm.pretrained.esm2_t33_650M_UR50D()\n        self.fc1 = nn.Linear(1280, HIDDEN_UNITS_POS_CONTACT)\n        self.fc2 = nn.Linear(HIDDEN_UNITS_POS_CONTACT, 1)\n    \n    \n    def _freeze_esm2_layers(self):\n        total_blocks = 33\n        initial_layers = 2\n        layers_per_block = 16\n        num_freeze_blocks = total_blocks - 3\n        for _, param in list(self.esm2.named_parameters())[\n            :initial_layers + layers_per_block * num_freeze_blocks]:\n            param.requires_grad = False\n            \n\n    def forward(self, wt_ids, mut_ids):\n        outputs1 = self.esm2.forward(wt_ids, repr_layers=[33])[\n            'representations'][33]\n        outputs2 = self.esm2.forward(mut_ids, repr_layers=[33])[\n            'representations'][33]\n        outputs1_mean = outputs1.mean(1)\n        outputs2_mean = outputs2.mean(1)\n        add = outputs1_mean + outputs2_mean\n        fc1_outputs = F.relu(self.fc1(add))\n        logits = self.fc2(fc1_outputs)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:36:27.293502Z","iopub.execute_input":"2023-08-12T12:36:27.294069Z","iopub.status.idle":"2023-08-12T12:36:27.306608Z","shell.execute_reply.started":"2023-08-12T12:36:27.294036Z","shell.execute_reply":"2023-08-12T12:36:27.305624Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"def train_2(train_dataloader, model, epochs):\n    model.train()\n    for _ in range(epochs):\n        tr_loss = 0\n        for batch in tqdm(train_dataloader):\n            wt_ids, mut_ids, _, labels = batch\n            wt_ids = wt_ids.squeeze(1).to(device)\n            mut_ids = mut_ids.squeeze(1).to(device)\n            labels = labels.to(device)\n            logits = model(wt_ids, mut_ids)\n            loss = torch.nn.functional.mse_loss(logits, labels)\n            tr_loss += loss.item()\n        \n            torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=0.1\n            )\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        epoch_loss = tr_loss / len(train_dataloader)\n        print(f\"Training loss epoch: {epoch_loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:34:07.355739Z","iopub.execute_input":"2023-08-12T12:34:07.356099Z","iopub.status.idle":"2023-08-12T12:34:07.366252Z","shell.execute_reply.started":"2023-08-12T12:34:07.356067Z","shell.execute_reply":"2023-08-12T12:34:07.364103Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"lr = 1e-5\nEPOCHS = 3","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:33:30.886313Z","iopub.execute_input":"2023-08-12T12:33:30.886717Z","iopub.status.idle":"2023-08-12T12:33:30.892458Z","shell.execute_reply.started":"2023-08-12T12:33:30.886683Z","shell.execute_reply":"2023-08-12T12:33:30.891231Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"model = ESM_sum_seqembed().to(device)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:36:34.222944Z","iopub.execute_input":"2023-08-12T12:36:34.223414Z","iopub.status.idle":"2023-08-12T12:37:00.321419Z","shell.execute_reply.started":"2023-08-12T12:36:34.223374Z","shell.execute_reply":"2023-08-12T12:37:00.320082Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"train_2(train_dataloader, model, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:37:09.815272Z","iopub.execute_input":"2023-08-12T12:37:09.815663Z","iopub.status.idle":"2023-08-12T12:37:12.174733Z","shell.execute_reply.started":"2023-08-12T12:37:09.815630Z","shell.execute_reply":"2023-08-12T12:37:12.173172Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stderr","text":"  0%|          | 0/112 [00:02<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[90], line 10\u001b[0m, in \u001b[0;36mtrain_2\u001b[0;34m(train_dataloader, model, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m mut_ids \u001b[38;5;241m=\u001b[39m mut_ids\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwt_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmut_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(logits, labels)\n\u001b[1;32m     12\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[92], line 21\u001b[0m, in \u001b[0;36mESM_sum_seqembed.forward\u001b[0;34m(self, wt_ids, mut_ids)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, wt_ids, mut_ids):\n\u001b[0;32m---> 21\u001b[0m     outputs1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwt_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m33\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m33\u001b[39m]\n\u001b[1;32m     23\u001b[0m     outputs2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mesm2\u001b[38;5;241m.\u001b[39mforward(mut_ids, repr_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m33\u001b[39m])[\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m33\u001b[39m]\n\u001b[1;32m     25\u001b[0m     outputs1_mean \u001b[38;5;241m=\u001b[39m outputs1\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/esm/model/esm2.py:112\u001b[0m, in \u001b[0;36mESM2.forward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    109\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 112\u001b[0m     x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m repr_layers:\n\u001b[1;32m    118\u001b[0m         hidden_representations[layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/esm/modules.py:138\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[1;32m    136\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    137\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(x)\n\u001b[0;32m--> 138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/esm/modules.py:24\u001b[0m, in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgelu\u001b[39m(x):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implementation of the gelu activation function.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    For information: OpenAI GPT's gelu is slightly different\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    (and gives slightly different results):\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39merf(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m))\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 14.76 GiB total capacity; 13.69 GiB already allocated; 17.75 MiB free; 14.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 14.76 GiB total capacity; 13.69 GiB already allocated; 17.75 MiB free; 14.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"y_pred, y_true = valid(model, valid_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T12:05:05.450038Z","iopub.execute_input":"2023-08-12T12:05:05.450790Z","iopub.status.idle":"2023-08-12T12:05:05.851585Z","shell.execute_reply.started":"2023-08-12T12:05:05.450754Z","shell.execute_reply":"2023-08-12T12:05:05.849305Z"},"trusted":true},"execution_count":43,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred, y_true \u001b[38;5;241m=\u001b[39m \u001b[43mvalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[42], line 10\u001b[0m, in \u001b[0;36mvalid\u001b[0;34m(model, valid_dataloader)\u001b[0m\n\u001b[1;32m      8\u001b[0m mut_ids \u001b[38;5;241m=\u001b[39m mut_ids\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwt_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmut_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     12\u001b[0m y_pred\u001b[38;5;241m.\u001b[39mappend(logits)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[20], line 23\u001b[0m, in \u001b[0;36mESM_concat_mut.forward\u001b[0;34m(self, wt_ids, mut_ids, pos)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, wt_ids, mut_ids, pos):\n\u001b[1;32m     21\u001b[0m     outputs1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mesm2\u001b[38;5;241m.\u001b[39mforward(wt_ids, repr_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m33\u001b[39m])[\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m33\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m     outputs2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmut_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m33\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m33\u001b[39m]\n\u001b[1;32m     25\u001b[0m     wt_pos \u001b[38;5;241m=\u001b[39m outputs1[:, pos, :]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     mut_pos \u001b[38;5;241m=\u001b[39m outputs2[:, pos, :]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/esm/model/esm2.py:112\u001b[0m, in \u001b[0;36mESM2.forward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    109\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 112\u001b[0m     x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m repr_layers:\n\u001b[1;32m    118\u001b[0m         hidden_representations[layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/esm/modules.py:138\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[1;32m    136\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    137\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(x)\n\u001b[0;32m--> 138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/esm/modules.py:24\u001b[0m, in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgelu\u001b[39m(x):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implementation of the gelu activation function.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    For information: OpenAI GPT's gelu is slightly different\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    (and gives slightly different results):\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39merf(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m))\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 13.73 GiB already allocated; 3.75 MiB free; 14.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 13.73 GiB already allocated; 3.75 MiB free; 14.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}